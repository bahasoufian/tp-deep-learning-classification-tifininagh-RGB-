{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f38f4d9-4550-41b4-85f0-46cb094948b0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '_name_' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 429\u001b[0m\n\u001b[0;32m    425\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m    427\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining completed successfully!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 429\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _name_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_main_\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    430\u001b[0m     main()\n",
      "\u001b[1;31mNameError\u001b[0m: name '_name_' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import os\n",
    "import cv2\n",
    "from scipy.ndimage import rotate\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class TifinaghMLP:\n",
    "    \"\"\"\n",
    "    Multi-Layer Perceptron for Tifinagh character classification\n",
    "    Architecture: Input(1024) -> Hidden1(64) -> Hidden2(32) -> Output(33)\n",
    "    \"\"\"\n",
    "    \n",
    "    def _init_(self, input_size=1024, hidden1_size=64, hidden2_size=32, output_size=33, \n",
    "                 learning_rate=0.01, lambda_reg=0.001):\n",
    "        self.input_size = input_size\n",
    "        self.hidden1_size = hidden1_size\n",
    "        self.hidden2_size = hidden2_size\n",
    "        self.output_size = output_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.lambda_reg = lambda_reg\n",
    "        \n",
    "        # Initialize weights and biases\n",
    "        self.initialize_parameters()\n",
    "        \n",
    "        # Adam optimizer parameters\n",
    "        self.beta1 = 0.9\n",
    "        self.beta2 = 0.999\n",
    "        self.epsilon = 1e-8\n",
    "        self.t = 0  # time step\n",
    "        self.initialize_adam()\n",
    "        \n",
    "        # Training history\n",
    "        self.train_losses = []\n",
    "        self.train_accuracies = []\n",
    "        self.val_losses = []\n",
    "        self.val_accuracies = []\n",
    "    \n",
    "    def initialize_parameters(self):\n",
    "        \"\"\"Initialize weights using Xavier initialization\"\"\"\n",
    "        np.random.seed(42)\n",
    "        \n",
    "        # Layer 1: Input -> Hidden1\n",
    "        self.W1 = np.random.randn(self.input_size, self.hidden1_size) * np.sqrt(2.0 / self.input_size)\n",
    "        self.b1 = np.zeros((1, self.hidden1_size))\n",
    "        \n",
    "        # Layer 2: Hidden1 -> Hidden2\n",
    "        self.W2 = np.random.randn(self.hidden1_size, self.hidden2_size) * np.sqrt(2.0 / self.hidden1_size)\n",
    "        self.b2 = np.zeros((1, self.hidden2_size))\n",
    "        \n",
    "        # Layer 3: Hidden2 -> Output\n",
    "        self.W3 = np.random.randn(self.hidden2_size, self.output_size) * np.sqrt(2.0 / self.hidden2_size)\n",
    "        self.b3 = np.zeros((1, self.output_size))\n",
    "    \n",
    "    def initialize_adam(self):\n",
    "        \"\"\"Initialize Adam optimizer parameters\"\"\"\n",
    "        # First moments (means)\n",
    "        self.mW1 = np.zeros_like(self.W1)\n",
    "        self.mb1 = np.zeros_like(self.b1)\n",
    "        self.mW2 = np.zeros_like(self.W2)\n",
    "        self.mb2 = np.zeros_like(self.b2)\n",
    "        self.mW3 = np.zeros_like(self.W3)\n",
    "        self.mb3 = np.zeros_like(self.b3)\n",
    "        \n",
    "        # Second moments (variances)\n",
    "        self.vW1 = np.zeros_like(self.W1)\n",
    "        self.vb1 = np.zeros_like(self.b1)\n",
    "        self.vW2 = np.zeros_like(self.W2)\n",
    "        self.vb2 = np.zeros_like(self.b2)\n",
    "        self.vW3 = np.zeros_like(self.W3)\n",
    "        self.vb3 = np.zeros_like(self.b3)\n",
    "    \n",
    "    def relu(self, z):\n",
    "        \"\"\"ReLU activation function\"\"\"\n",
    "        return np.maximum(0, z)\n",
    "    \n",
    "    def relu_derivative(self, z):\n",
    "        \"\"\"Derivative of ReLU function\"\"\"\n",
    "        return (z > 0).astype(float)\n",
    "    \n",
    "    def softmax(self, z):\n",
    "        \"\"\"Softmax activation function with numerical stability\"\"\"\n",
    "        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "    \n",
    "    def forward_propagation(self, X):\n",
    "        \"\"\"Forward propagation through the network\"\"\"\n",
    "        # Layer 1\n",
    "        self.Z1 = np.dot(X, self.W1) + self.b1\n",
    "        self.A1 = self.relu(self.Z1)\n",
    "        \n",
    "        # Layer 2\n",
    "        self.Z2 = np.dot(self.A1, self.W2) + self.b2\n",
    "        self.A2 = self.relu(self.Z2)\n",
    "        \n",
    "        # Layer 3 (Output)\n",
    "        self.Z3 = np.dot(self.A2, self.W3) + self.b3\n",
    "        self.A3 = self.softmax(self.Z3)\n",
    "        \n",
    "        return self.A3\n",
    "    \n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        \"\"\"Compute cross-entropy loss with L2 regularization\"\"\"\n",
    "        m = y_true.shape[0]\n",
    "        \n",
    "        # Cross-entropy loss\n",
    "        cross_entropy = -np.sum(y_true * np.log(y_pred + 1e-15)) / m\n",
    "        \n",
    "        # L2 regularization\n",
    "        l2_reg = (self.lambda_reg / (2 * m)) * (\n",
    "            np.sum(self.W1*2) + np.sum(self.W22) + np.sum(self.W3*2)\n",
    "        )\n",
    "        \n",
    "        return cross_entropy + l2_reg\n",
    "    \n",
    "    def compute_accuracy(self, y_true, y_pred):\n",
    "        \"\"\"Compute classification accuracy\"\"\"\n",
    "        y_true_labels = np.argmax(y_true, axis=1)\n",
    "        y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "        return np.mean(y_true_labels == y_pred_labels)\n",
    "    \n",
    "    def backward_propagation(self, X, y_true, y_pred):\n",
    "        \"\"\"Backward propagation to compute gradients\"\"\"\n",
    "        m = X.shape[0]\n",
    "        \n",
    "        # Output layer gradients\n",
    "        dZ3 = y_pred - y_true\n",
    "        dW3 = (1/m) * np.dot(self.A2.T, dZ3) + (self.lambda_reg/m) * self.W3\n",
    "        db3 = (1/m) * np.sum(dZ3, axis=0, keepdims=True)\n",
    "        \n",
    "        # Hidden layer 2 gradients\n",
    "        dA2 = np.dot(dZ3, self.W3.T)\n",
    "        dZ2 = dA2 * self.relu_derivative(self.Z2)\n",
    "        dW2 = (1/m) * np.dot(self.A1.T, dZ2) + (self.lambda_reg/m) * self.W2\n",
    "        db2 = (1/m) * np.sum(dZ2, axis=0, keepdims=True)\n",
    "        \n",
    "        # Hidden layer 1 gradients\n",
    "        dA1 = np.dot(dZ2, self.W2.T)\n",
    "        dZ1 = dA1 * self.relu_derivative(self.Z1)\n",
    "        dW1 = (1/m) * np.dot(X.T, dZ1) + (self.lambda_reg/m) * self.W1\n",
    "        db1 = (1/m) * np.sum(dZ1, axis=0, keepdims=True)\n",
    "        \n",
    "        return dW1, db1, dW2, db2, dW3, db3\n",
    "    \n",
    "    def update_parameters_adam(self, dW1, db1, dW2, db2, dW3, db3):\n",
    "        \"\"\"Update parameters using Adam optimizer\"\"\"\n",
    "        self.t += 1\n",
    "        \n",
    "        # Update first moments\n",
    "        self.mW1 = self.beta1 * self.mW1 + (1 - self.beta1) * dW1\n",
    "        self.mb1 = self.beta1 * self.mb1 + (1 - self.beta1) * db1\n",
    "        self.mW2 = self.beta1 * self.mW2 + (1 - self.beta1) * dW2\n",
    "        self.mb2 = self.beta1 * self.mb2 + (1 - self.beta1) * db2\n",
    "        self.mW3 = self.beta1 * self.mW3 + (1 - self.beta1) * dW3\n",
    "        self.mb3 = self.beta1 * self.mb3 + (1 - self.beta1) * db3\n",
    "        \n",
    "        # Update second moments\n",
    "        self.vW1 = self.beta2 * self.vW1 + (1 - self.beta2) * (dW1**2)\n",
    "        self.vb1 = self.beta2 * self.vb1 + (1 - self.beta2) * (db1**2)\n",
    "        self.vW2 = self.beta2 * self.vW2 + (1 - self.beta2) * (dW2**2)\n",
    "        self.vb2 = self.beta2 * self.vb2 + (1 - self.beta2) * (db2**2)\n",
    "        self.vW3 = self.beta2 * self.vW3 + (1 - self.beta2) * (dW3**2)\n",
    "        self.vb3 = self.beta2 * self.vb3 + (1 - self.beta2) * (db3**2)\n",
    "        \n",
    "        # Bias correction\n",
    "        mW1_corrected = self.mW1 / (1 - self.beta1**self.t)\n",
    "        mb1_corrected = self.mb1 / (1 - self.beta1**self.t)\n",
    "        mW2_corrected = self.mW2 / (1 - self.beta1**self.t)\n",
    "        mb2_corrected = self.mb2 / (1 - self.beta1**self.t)\n",
    "        mW3_corrected = self.mW3 / (1 - self.beta1**self.t)\n",
    "        mb3_corrected = self.mb3 / (1 - self.beta1**self.t)\n",
    "        \n",
    "        vW1_corrected = self.vW1 / (1 - self.beta2**self.t)\n",
    "        vb1_corrected = self.vb1 / (1 - self.beta2**self.t)\n",
    "        vW2_corrected = self.vW2 / (1 - self.beta2**self.t)\n",
    "        vb2_corrected = self.vb2 / (1 - self.beta2**self.t)\n",
    "        vW3_corrected = self.vW3 / (1 - self.beta2**self.t)\n",
    "        vb3_corrected = self.vb3 / (1 - self.beta2**self.t)\n",
    "        \n",
    "        # Update parameters\n",
    "        self.W1 -= self.learning_rate * mW1_corrected / (np.sqrt(vW1_corrected) + self.epsilon)\n",
    "        self.b1 -= self.learning_rate * mb1_corrected / (np.sqrt(vb1_corrected) + self.epsilon)\n",
    "        self.W2 -= self.learning_rate * mW2_corrected / (np.sqrt(vW2_corrected) + self.epsilon)\n",
    "        self.b2 -= self.learning_rate * mb2_corrected / (np.sqrt(vb2_corrected) + self.epsilon)\n",
    "        self.W3 -= self.learning_rate * mW3_corrected / (np.sqrt(vW3_corrected) + self.epsilon)\n",
    "        self.b3 -= self.learning_rate * mb3_corrected / (np.sqrt(vb3_corrected) + self.epsilon)\n",
    "    \n",
    "    def train(self, X_train, y_train, X_val=None, y_val=None, epochs=100, verbose=True):\n",
    "        \"\"\"Train the neural network\"\"\"\n",
    "        for epoch in range(epochs):\n",
    "            # Forward propagation\n",
    "            y_pred_train = self.forward_propagation(X_train)\n",
    "            \n",
    "            # Compute loss and accuracy\n",
    "            train_loss = self.compute_loss(y_train, y_pred_train)\n",
    "            train_acc = self.compute_accuracy(y_train, y_pred_train)\n",
    "            \n",
    "            # Backward propagation\n",
    "            dW1, db1, dW2, db2, dW3, db3 = self.backward_propagation(X_train, y_train, y_pred_train)\n",
    "            \n",
    "            # Update parameters using Adam\n",
    "            self.update_parameters_adam(dW1, db1, dW2, db2, dW3, db3)\n",
    "            \n",
    "            # Store training metrics\n",
    "            self.train_losses.append(train_loss)\n",
    "            self.train_accuracies.append(train_acc)\n",
    "            \n",
    "            # Validation metrics\n",
    "            if X_val is not None and y_val is not None:\n",
    "                y_pred_val = self.forward_propagation(X_val)\n",
    "                val_loss = self.compute_loss(y_val, y_pred_val)\n",
    "                val_acc = self.compute_accuracy(y_val, y_pred_val)\n",
    "                self.val_losses.append(val_loss)\n",
    "                self.val_accuracies.append(val_acc)\n",
    "                \n",
    "                if verbose and (epoch + 1) % 10 == 0:\n",
    "                    print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "            else:\n",
    "                if verbose and (epoch + 1) % 10 == 0:\n",
    "                    print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions\"\"\"\n",
    "        return self.forward_propagation(X)\n",
    "    \n",
    "    def plot_training_history(self):\n",
    "        \"\"\"Plot training history\"\"\"\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "        \n",
    "        # Loss plot\n",
    "        ax1.plot(self.train_losses, label='Training Loss')\n",
    "        if self.val_losses:\n",
    "            ax1.plot(self.val_losses, label='Validation Loss')\n",
    "        ax1.set_title('Model Loss')\n",
    "        ax1.set_xlabel('Epoch')\n",
    "        ax1.set_ylabel('Loss')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True)\n",
    "        \n",
    "        # Accuracy plot\n",
    "        ax2.plot(self.train_accuracies, label='Training Accuracy')\n",
    "        if self.val_accuracies:\n",
    "            ax2.plot(self.val_accuracies, label='Validation Accuracy')\n",
    "        ax2.set_title('Model Accuracy')\n",
    "        ax2.set_xlabel('Epoch')\n",
    "        ax2.set_ylabel('Accuracy')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "class DataAugmentation:\n",
    "    \"\"\"Data augmentation techniques for image data\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def rotate_image(image, angle):\n",
    "        \"\"\"Rotate image by given angle\"\"\"\n",
    "        return rotate(image, angle, reshape=False, mode='constant', cval=0)\n",
    "    \n",
    "    @staticmethod\n",
    "    def translate_image(image, shift_x, shift_y):\n",
    "        \"\"\"Translate image by given shifts\"\"\"\n",
    "        rows, cols = image.shape\n",
    "        M = np.float32([[1, 0, shift_x], [0, 1, shift_y]])\n",
    "        return cv2.warpAffine(image, M, (cols, rows))\n",
    "    \n",
    "    @staticmethod\n",
    "    def add_noise(image, noise_factor=0.1):\n",
    "        \"\"\"Add Gaussian noise to image\"\"\"\n",
    "        noise = np.random.normal(0, noise_factor, image.shape)\n",
    "        return np.clip(image + noise, 0, 1)\n",
    "    \n",
    "    @staticmethod\n",
    "    def augment_dataset(X, y, augmentation_factor=2):\n",
    "        \"\"\"Augment dataset with rotations and translations\"\"\"\n",
    "        X_augmented = [X]\n",
    "        y_augmented = [y]\n",
    "        \n",
    "        for _ in range(augmentation_factor):\n",
    "            X_aug = []\n",
    "            for i in range(X.shape[0]):\n",
    "                # Reshape to 32x32 for augmentation\n",
    "                img = X[i].reshape(32, 32)\n",
    "                \n",
    "                # Random rotation (-15 to 15 degrees)\n",
    "                angle = np.random.uniform(-15, 15)\n",
    "                img_rot = DataAugmentation.rotate_image(img, angle)\n",
    "                \n",
    "                # Random translation (-2 to 2 pixels)\n",
    "                shift_x = np.random.randint(-2, 3)\n",
    "                shift_y = np.random.randint(-2, 3)\n",
    "                img_trans = DataAugmentation.translate_image(img_rot, shift_x, shift_y)\n",
    "                \n",
    "                # Add noise\n",
    "                img_noise = DataAugmentation.add_noise(img_trans, 0.05)\n",
    "                \n",
    "                X_aug.append(img_noise.flatten())\n",
    "            \n",
    "            X_augmented.append(np.array(X_aug))\n",
    "            y_augmented.append(y)\n",
    "        \n",
    "        return np.vstack(X_augmented), np.vstack(y_augmented)\n",
    "\n",
    "def load_amhcd_dataset(data_path):\n",
    "    \"\"\"\n",
    "    Load AMHCD dataset\n",
    "    Note: This is a placeholder function. You need to adapt it to your actual data structure.\n",
    "    \"\"\"\n",
    "    # This is a synthetic example - replace with actual data loading\n",
    "    print(\"Loading AMHCD dataset...\")\n",
    "    \n",
    "    # For demonstration, create synthetic data\n",
    "    # In practice, you would load from the actual AMHCD dataset\n",
    "    np.random.seed(42)\n",
    "    n_samples = 25740  # 780 samples per 33 characters\n",
    "    X = np.random.rand(n_samples, 1024)  # 32x32 flattened images\n",
    "    y = np.random.randint(0, 33, n_samples)  # 33 classes\n",
    "    \n",
    "    # Convert to one-hot encoding\n",
    "    y_onehot = np.zeros((n_samples, 33))\n",
    "    y_onehot[np.arange(n_samples), y] = 1\n",
    "    \n",
    "    return X, y_onehot, y\n",
    "\n",
    "def cross_validation_evaluation(X, y, k_folds=5):\n",
    "    \"\"\"Perform k-fold cross validation\"\"\"\n",
    "    kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "    cv_scores = []\n",
    "    \n",
    "    print(f\"Performing {k_folds}-fold cross validation...\")\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
    "        print(f\"Fold {fold + 1}/{k_folds}\")\n",
    "        \n",
    "        X_train_fold, X_val_fold = X[train_idx], X[val_idx]\n",
    "        y_train_fold, y_val_fold = y[train_idx], y[val_idx]\n",
    "        \n",
    "        # Create and train model\n",
    "        model = TifinaghMLP()\n",
    "        model.train(X_train_fold, y_train_fold, X_val_fold, y_val_fold, \n",
    "                   epochs=50, verbose=False)\n",
    "        \n",
    "        # Evaluate\n",
    "        y_pred = model.predict(X_val_fold)\n",
    "        accuracy = model.compute_accuracy(y_val_fold, y_pred)\n",
    "        cv_scores.append(accuracy)\n",
    "        \n",
    "        print(f\"Fold {fold + 1} Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    print(f\"Cross-validation scores: {cv_scores}\")\n",
    "    print(f\"Mean CV Accuracy: {np.mean(cv_scores):.4f} (+/- {np.std(cv_scores) * 2:.4f})\")\n",
    "    \n",
    "    return cv_scores\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    print(\"=== Tifinagh Character Classification with Neural Networks ===\")\n",
    "    \n",
    "    # Load dataset\n",
    "    X, y_onehot, y_labels = load_amhcd_dataset(\"path/to/amhcd\")\n",
    "    \n",
    "    # Normalize data\n",
    "    X = X / 255.0  # Assuming pixel values are 0-255\n",
    "    \n",
    "    print(f\"Dataset shape: {X.shape}\")\n",
    "    print(f\"Number of classes: {y_onehot.shape[1]}\")\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y_onehot, test_size=0.2, random_state=42, stratify=y_labels\n",
    "    )\n",
    "    \n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train, y_train, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    print(f\"Training set shape: {X_train.shape}\")\n",
    "    print(f\"Validation set shape: {X_val.shape}\")\n",
    "    print(f\"Test set shape: {X_test.shape}\")\n",
    "    \n",
    "    # Data augmentation\n",
    "    print(\"Applying data augmentation...\")\n",
    "    X_train_aug, y_train_aug = DataAugmentation.augment_dataset(X_train, y_train, 1)\n",
    "    print(f\"Augmented training set shape: {X_train_aug.shape}\")\n",
    "    \n",
    "    # Create and train model\n",
    "    print(\"Training neural network...\")\n",
    "    model = TifinaghMLP(learning_rate=0.001, lambda_reg=0.001)\n",
    "    model.train(X_train_aug, y_train_aug, X_val, y_val, epochs=100, verbose=True)\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    print(\"Evaluating on test set...\")\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    test_accuracy = model.compute_accuracy(y_test, y_pred_test)\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "    \n",
    "    # Plot training history\n",
    "    model.plot_training_history()\n",
    "    \n",
    "    # Cross-validation evaluation\n",
    "    cv_scores = cross_validation_evaluation(X, y_onehot, k_folds=5)\n",
    "    \n",
    "    # Classification report\n",
    "    y_test_labels = np.argmax(y_test, axis=1)\n",
    "    y_pred_labels = np.argmax(y_pred_test, axis=1)\n",
    "    \n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test_labels, y_pred_labels))\n",
    "    \n",
    "    # Confusion matrix\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    cm = confusion_matrix(y_test_labels, y_pred_labels)\n",
    "    sns.heatmap(cm, annot=False, fmt='d', cmap='Blues')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Training completed successfully!\")\n",
    "\n",
    "if _name_ == \"_main_\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64f71d9-c761-4c4f-819d-d8208925951e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
